{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hand Recognition",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "6w_xFBGE0iF_",
        "LoZGkQTy-HrW",
        "37MCrtPLtt5Z",
        "iWawD9RX--Xa"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Benned-H/Reading_List/blob/master/Notes/Hand_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6w_xFBGE0iF_",
        "colab_type": "text"
      },
      "source": [
        "# 1. Hand Segmentation Using Skin Color and Background Information\n",
        "By Wei Wang, Jing Pan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Svqbo61i1Bbg",
        "colab_type": "text"
      },
      "source": [
        "This paper presents a new method for segmenting hands from the background of an image. Their method uses an adaptive skin color model with three steps:\n",
        "1. Capture pixel values of hand and background\n",
        "2. Propose Gaussian models on the color space\n",
        "3. Segment the image using various models, intersect the results\n",
        "\n",
        "Results were better than other skin-color-only models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0XNO_C11xaW",
        "colab_type": "text"
      },
      "source": [
        "## 1. Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgkU6w3N1yeE",
        "colab_type": "text"
      },
      "source": [
        "Various applications make accurate hand quite important, and segmentation is a crucial first step in this process. Because human skin is generally within a limited range of hues, color-based hand recognition has been investigated for decades. The process depends on two choices: the **color space** and the **model of distribution** for skin colors. Prior work used a variety of techniques and spaces, including:\n",
        "* Color spaces: Normalized RGB, CIE, XYZ, HSV, HSI, YCbCr\n",
        "* Gaussian model for distributions\n",
        "* Edge detection\n",
        "* Varied chrominance spaces\n",
        "* Skin/edge information in various spaces"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63PWGpyE4IDM",
        "colab_type": "text"
      },
      "source": [
        "## 2. Color Space and Gaussian model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqhkQsuQ4N_D",
        "colab_type": "text"
      },
      "source": [
        "Their method was primarily concerned with the use of background information to help segmentation. Thus they only used the normalized RGB and YCbCr color spaces and a single Gaussian model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_0v0skX4oPk",
        "colab_type": "text"
      },
      "source": [
        "**Normalized RGB**   \n",
        "RGB is a convenient color model widely used for processing image data. Unfortunately, the RGB color space is not robust because it cannot define the same color in different conditions or illumination. Normalized RGB was proposed to help this problem, and indeed gets better performance under different light conditions *only in uniform illumination*. Normalized RGB can be calculated as:\n",
        "\n",
        "$R=\\frac{R}{R+G+B}$; $G=\\frac{G}{R+G+B}$; $B=\\frac{B}{R+G+B}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ModQqCxF5_MK",
        "colab_type": "text"
      },
      "source": [
        "**YCbCr**   \n",
        "YCbCr is considered to be better for our purposes than RGB. The clustering is better, it's easy to calculate, and has far less overlap between skin and non-skin tones in various illumination conditions. YCbCr separates out a luminance signal (Y) and two chrominance components (Cb and Cr). We can discard signal Y to improve performance over various lighting conditions. The transform from RGB to YCbCr is simple:\n",
        "\n",
        "$\n",
        "\\begin{bmatrix}\n",
        "Y \\\\ Cb \\\\ Cr\n",
        "\\end{bmatrix}=\n",
        "\\begin{bmatrix}\n",
        "0.2568 & 0.5041 & 0.0979 \\\\\n",
        "-0.1482 & -0.2910 & 0.4392 \\\\\n",
        "0.4392 & -0.3678 & -0.0714\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "R \\\\ G \\\\ B\n",
        "\\end{bmatrix}+\n",
        "\\begin{bmatrix}\n",
        "16 \\\\ 128 \\\\ 128\n",
        "\\end{bmatrix}\n",
        "$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoZGkQTy-HrW",
        "colab_type": "text"
      },
      "source": [
        "## Gaussian Mixture Model - [Brilliant](https://brilliant.org/wiki/gaussian-mixture-model/#)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfWB9RygQ3vC",
        "colab_type": "text"
      },
      "source": [
        "Gaussian mixture models (GMMs) are a probabilistic model for representing normally distributed subpopulations within an overall population (a normal distribution has mean = median = mode, and symmetry over its center). Mixture models don't need to know which subpopulation each data point belongs to, which make them somewhat unsupervised learning (e.g. human height data would have two normal distributions between the sexes, which a GMM could capture)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKghJ421SXjP",
        "colab_type": "text"
      },
      "source": [
        "**Motivation**   \n",
        "We might want to try modeling data with a GMM if it appears to have more than one 'peak' distribution. Unimodal (one 'peak') models would give a poor fit in such a case, and yet GMMs retain the computational benefits of a single Gaussian model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uInoVjDWUPft",
        "colab_type": "text"
      },
      "source": [
        "**To be continued upon additional probability background...**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lumBg1GU2Jj",
        "colab_type": "text"
      },
      "source": [
        "## 2 cont."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eQ9fjriU3pn",
        "colab_type": "text"
      },
      "source": [
        "The properties of skin color can be modeled using a Gaussian distribution, which has the formula:   \n",
        "$f(x)=\\frac{1}{\\sqrt{2\\pi \\sigma ^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$, where $\\mu$ is the mean value of the samples and $\\sigma$ is the variance value.\n",
        "\n",
        "Using this model for skin color is a process of matching each pixel in the image. If matched, we consider the pixel as a skin pixel, and if not we consider it background. The two parameters ($\\mu$ and $\\sigma$) decide the structure of our Gaussian model, and need to be learned. A common method for this is **offline training** on thousands of images, but these authors use an adaptive skin color model, which uses the center of the hand skin to calculate and constantly update the parameters of the model. This **online** model seems to work better in different illuminations. Because this paper doesn't explain that model, I'll read through the source of this idea:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37MCrtPLtt5Z",
        "colab_type": "text"
      },
      "source": [
        "# 2. A New Method for Hand Segmentation Using Free-Form Skin Color Model\n",
        "By Ahmad Yahya Dawod, Junaidi Abdullah, and Md.Jahangir Alam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQhQnCYjt43Z",
        "colab_type": "text"
      },
      "source": [
        "Segmentation remains difficult; this paper proposes a new method using a free-form skin color model. The pixel values of the hand are represented in the YbCbCr color space, and the CbCr space is mapped to a CbCr plane. To cluster the region of skin color on this plane, edge detection is used (as opposed to an ellipse) to construct a free-form skin color model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZ2gqHmXu5MK",
        "colab_type": "text"
      },
      "source": [
        "## I. Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFkIMhequ6Lk",
        "colab_type": "text"
      },
      "source": [
        "The goal of hand segmentation is to detect the position and orientation of hands in an image; the aim of skin color pixel classification is to determine if a color pixel is a skin color or non-skin color. There are several techniques used to model the skin color:\n",
        "* An elliptical boundary model which fits an ellipse on the CbCr plane. The ellipse ends up including non-skin pixel colors, unfortunately.\n",
        "* Coarse model - Fixed straight lines are used as boundaries to a coarse region, but again this includes non-skin pixels.\n",
        "* Estimate the boundary by constructing bilinear and bicubic boxes around the CbCr pixel cluster. Same issue.\n",
        "\n",
        "This paper proposes a new method that uses a free-form boundary which models the skin color depending on the person and minimizes the inclusion of non-skin pixels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3MbdIsWwob9",
        "colab_type": "text"
      },
      "source": [
        "## II. Suggested Method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQQC4xgawtV9",
        "colab_type": "text"
      },
      "source": [
        "Their method consists of four modules:\n",
        "1. Image acquisition (skin region cropping)\n",
        "2. Mapping (CbCr color space mapping)\n",
        "3. Morphology (erosion & dilation)\n",
        "4. Boundary creation (detecting edges)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQPF2yfhxKeg",
        "colab_type": "text"
      },
      "source": [
        "**Image Acquisition**   \n",
        "Because different people have different skin tones, the authors believe (and I agree) that we shouldn't just define some general range for skin tones. Thus we need to crop a skin image of the person using the system to develop a free-form model specific to them. As has been mentioned, choosing the right color space is the first step we need to tackle. Long story short, we choose YCbCr for the previously written reasons (taken from here, by the way). Skin image cropping just crops the image so we only see a patch of the user's skin as the cropped result. We can then form a cluster in our color space with this example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2Nfm5TiynPy",
        "colab_type": "text"
      },
      "source": [
        "**CbCr Color Space Mapping**   \n",
        "They observed that the intensity value Y of YCbCr has little influence on the color distribution. On the Cb and Cr plane, we can generate a map where white (255) points are skin pixels and black (0) are non-skin. The resulting 255x255 map is the range of skin color present in the cropped image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-894vL3zqU0",
        "colab_type": "text"
      },
      "source": [
        "**Morphology**   \n",
        "This stage uses image processing to create a cleaner single free-form shape. Two operations are used: Dilation adds pixels to fill in missing pixels in the white cluster and erosion removes extra pixels not belonging to the white cluster. Both help our resulting segmentation, and are applied in the order of dilation, erosion, and then edge point extraction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3jnx4A_1rYK",
        "colab_type": "text"
      },
      "source": [
        "**Boundary Creation**   \n",
        "Here we determine the actual region which will define skin and non-skin color pixels. We consider the white cluster and apply an edge detection algorithm, for which there are a few different methods (gradient, laplacian). The **gradient method** detects the edge by looking for the maximum and minimum in the first derivative of the image, whereas the **laplacian method** searches for zero crossings in the second derivative of the image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tL5kUyZz3BNm",
        "colab_type": "text"
      },
      "source": [
        "For gradient edge detection, given image function $f(x,y)$, the gradient magnitude $g(x,y)$ and direction $\\theta(x,y)$ are computed as:   \n",
        "$g(x,y)\\cong \\sqrt{\\Delta x^2 + \\Delta y^2}$ and $\\theta(x,y)\\cong a \\tan(\\frac{\\Delta y}{\\Delta x})$,   \n",
        "where $\\Delta x =f(x+n,y)-f(x-n,y)$ and $\\Delta y =f(x,y+n)-f(x,y-n)$,   \n",
        "where $n$ is a small integer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BB0o2QQH42BF",
        "colab_type": "text"
      },
      "source": [
        "They used the Sobel operator for the gradient method due to its fast speed, fine scale edge detection, and smoothing action. Once the edge has been detected, they arrive at the skin color cluster. Pixels with color in the cluster are skin, those not are classified as non-skin."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSUAVvVC7vWB",
        "colab_type": "text"
      },
      "source": [
        "## III. Experimental Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbwHBMhm7yMe",
        "colab_type": "text"
      },
      "source": [
        "The authors also implemented a few morphological operations on the detected hand: fill holes in the segmented hand and also index all white regions in descending order by number of pixels. Only the white region with the most pixels is marked as white in the final result. In another example, a watch cuts the hand at the wrist, and with this knowledge the authors can include the top two white regions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHleuMOQ8nMH",
        "colab_type": "text"
      },
      "source": [
        "To validate their work, the authors estimated a **true positive rate** (TPR) and **false positive rate** (FPR). These were calculated as:   \n",
        "$\\text{TPR}=\\frac{\\text{TP}}{\\text{TP}+\\text{FN}}$, $\\text{FPR}=\\frac{\\text{FP}}{\\text{FP}+\\text{TN}}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NhDYIkU9egP",
        "colab_type": "text"
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRAJX9UG9f8K",
        "colab_type": "text"
      },
      "source": [
        "This paper introduced a novel free-form skin color model in the YCbCr color space. Its most important contribution is the ability to accurately tailor a hand segmentation color space to a user, regardless of their skin color or the setting's illumination.\n",
        "\n",
        "*--Done 6/23/2019--*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWawD9RX--Xa",
        "colab_type": "text"
      },
      "source": [
        "# Resuming 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zYBauBl_HFq",
        "colab_type": "text"
      },
      "source": [
        "## 3. Proposed Method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpZ4jgVF_IvE",
        "colab_type": "text"
      },
      "source": [
        "Their model introduces a new technique based on mixing a background model and a skin color model. A flowchart of the process:   \n",
        "![Flowchart](https://i.imgur.com/m7YG08D.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwCt7P1WAdgG",
        "colab_type": "text"
      },
      "source": [
        "A prerequisite of the process is that we've detected a hand and have a region of interest (ROI) that's a cropped region fully containing the hand and some background as well. In this ROI, we crop a region $P_0$ that contains *only skin color* (in the center of the hand, probably), as we needed in the previously read paper. We also crop $N$ regions $(P_1, P_2, ... ,P_N)$ containing random samples of background color. These may include some skin color at this point, no problem. We build six directions of the model to best ensure an accurate skin color and background color sampling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "le6Ji66QF06I",
        "colab_type": "text"
      },
      "source": [
        "We now want to check whether the background samples actually belong to the background. We calculate the mean ($M_i$) and variance value ($s_i$) of each cropped region to set up their Gaussian models. Each background crop is then used to segment the skin crop region with an automatic threshold interval $T_i$. Pixels in the skin crop region among $T_i$ are set to white (255) and others are set to black (0). If the proportion of the white parts exceeds some fixed threshold $S$, the corresponding background image is considered truth background and is accepted, otherwise it's discarded. To ensure a mostly-uniform background color, background crop regions with variance above some value $V_m$ are also dropped."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZlr9Bu2F5zp",
        "colab_type": "text"
      },
      "source": [
        "The skin crop regions and surviving background crop regions are used to segment the cropped image witht threshold $T_0$ and $T_1$, respectively. The segmentation results $(R_0,R_1,...,R_N)$ are intersected to get the final result. Their method used parameters $N=50$, $V_m=0.15$, $S=0.95$, and interval $T_i$ for the background crop region depended on $s_i$ to guarantee that 90% of the pixels in the crop region were in its Gaussian distribution. The original images were 320x240 and the cropped images varied in size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iIejrqyHJus",
        "colab_type": "text"
      },
      "source": [
        "## 4. Experimental Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wKUjSO3HLr2",
        "colab_type": "text"
      },
      "source": [
        "Their method worked far better than the traditional only-skin-based method. In comparing different values for $N$, increasing the value helps the method up to some point. When $N$ exceeds about 40, improvements slow. Perhaps $N$ only needs to be large enough that all useful background information is obtained? A smaller $N$ is preferable for images with simple backgrounds, but no description of how to measure this is given."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6ZzfLBuIX1V",
        "colab_type": "text"
      },
      "source": [
        "The authors evaluate the accuracy rate (AR) using the formula:   \n",
        "$\\text{AR}=\\frac{\\text{AS}\\cap \\text{AL}}{\\text{AS}\\cup \\text{AL}}$, where $\\text{AS}$ is the area of the segmented hand and $\\text{AL}$ is the area of the labeled hand in the image. Thus $\\text{AR}<1$ and a higher AR indicates a more accurate segmentation. Their method reaches an AR of 90.36% on a difficult image while the online model reached 69.30%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ls74Rp33l6x",
        "colab_type": "text"
      },
      "source": [
        "## 5. Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqZdzpdQ3ngh",
        "colab_type": "text"
      },
      "source": [
        "This paper introduced a hand segmentation method which used background information samples to improve accuracy. They used only a Gaussian model to characterize skin color in the RGB and YCbCr color spaces. Future work might include a GMM or free-skin color model to make further improvements.   \n",
        "*--Done 6/24/2019--*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFpsQL5l8LYq",
        "colab_type": "text"
      },
      "source": [
        "# 3. Dilation\n",
        "By R. Fisher, S. Perkins, A. Walker, and E. Wolfart"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-RBWY_lFV18",
        "colab_type": "text"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTj-i1Lz8OW0",
        "colab_type": "text"
      },
      "source": [
        "Dilation is one of the two basic operators of **mathematical morphology**, which concerns the theory of processing geometrical structures (typically applied to digital images). It's typically applied to binary images, and the basic effect is to gradually enlarge the boundaries of the **foreground** (white) pixels in the image. Thus the areas of white grow in size and holes in those regions shrink."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ir-Kl8DcIvwb",
        "colab_type": "text"
      },
      "source": [
        "## How It Works"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uC35BvScIw3D",
        "colab_type": "text"
      },
      "source": [
        "Dilation uses two inputs: the image to be dilated, and a **structuring element**. The structuring element (AKA kernel) is a set of coordinate points that determines the precise result of the dilation operation. The mathematical definition:   \n",
        "\n",
        "*Def*: Suppose $X$ is the set of Euclidean coordinates corresponding to the input binary image, and $K$ is the set of coordinates for the structuring element. Let $Kx$ denote the translation of $K$ so that its origin is at $x \\in X$. Then the dilation of $X$ by $K$ is simply the set of all points $x$ s.t. the intersection of $Kx$ with $X$ is non-empty.   \n",
        "*In my words*: For our result $Y \\subseteq X$, when we put kernel $K$ at each $y \\in Y$, $K$ will intersect with at least one $x \\in X$. I hope this is a correct understanding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vat8xftbKI9L",
        "colab_type": "text"
      },
      "source": [
        "E.g. Let's say our structuring element is a 3x3 square, with origin at its center. We'll represent foreground pixels with 1's and background as 0's."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6w3HufE4MEtc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "d8a7aa6e-4023-4f71-8391-4c66af9bb075"
      },
      "source": [
        "#  Let's say our image is the following 10x10 pattern:\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "image = np.array([[1,0,0,0,0,0,0,0,0,0],\n",
        "                [0,1,0,0,0,0,0,0,0,0],\n",
        "                [0,0,1,0,0,0,0,0,0,0],\n",
        "                [0,0,0,1,0,0,0,0,0,0],\n",
        "                [0,0,0,0,1,0,0,0,0,0],\n",
        "                [0,0,0,0,0,1,0,0,0,0],\n",
        "                [0,0,0,0,0,0,1,0,0,0],\n",
        "                [0,0,0,0,0,0,0,1,0,0],\n",
        "                [0,0,0,0,0,0,0,0,1,0],\n",
        "                [0,0,0,0,0,0,0,0,0,1]\n",
        "               ])\n",
        "plt.imshow(image)\n",
        "plt.show()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD8CAYAAABaQGkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKjmf3NcM06a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "d184b54a-9c8d-4258-d85f-b4ccd5c02f25"
      },
      "source": [
        "# Our kernel:\n",
        "kernel = np.array([[1,1,1],[1,1,1],[1,1,1]])\n",
        "plt.imshow(kernel)\n",
        "plt.show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQ8AAAD8CAYAAABpXiE9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n3qfruUEissEAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VZWaqHRMzzD",
        "colab_type": "text"
      },
      "source": [
        "In order to compute the dilation of this binary image, we consider each background pixel of the image in turn. We then superimpose the kernel over each of these **input pixels**. If *at least one* of the kernel's pixels overlaps with a foreground pixel, the input pixel is set to the foreground (probably do this *after* we first consider each background pixel). Thus if all pixels were set to background when we start, nothing will change."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o12ac4xEOLTv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dilate(image, kernel, kernel_origin):\n",
        "  # Dilate the given image using the given kernel.\n",
        "  for r,row in enumerate(image):\n",
        "    for c,col in r:\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8eVfTHGOgSF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dilate(image,1,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9E-wJVviOb7W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "d6753aa2-c46b-4df5-db8e-c25577df8b85"
      },
      "source": [
        "for r in image:\n",
        "  print(r)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 0 0 0 0 0 0 0 0 0]\n",
            "[0 1 0 0 0 0 0 0 0 0]\n",
            "[0 0 1 0 0 0 0 0 0 0]\n",
            "[0 0 0 1 0 0 0 0 0 0]\n",
            "[0 0 0 0 1 0 0 0 0 0]\n",
            "[0 0 0 0 0 1 0 0 0 0]\n",
            "[0 0 0 0 0 0 1 0 0 0]\n",
            "[0 0 0 0 0 0 0 1 0 0]\n",
            "[0 0 0 0 0 0 0 0 1 0]\n",
            "[0 0 0 0 0 0 0 0 0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAd-766T-ngj",
        "colab_type": "text"
      },
      "source": [
        "# To Read/Learn:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbiBJ7NV-pN5",
        "colab_type": "text"
      },
      "source": [
        "* https://wolfcrow.com/understanding-luminance-and-chrominance/\n",
        "* https://homepages.inf.ed.ac.uk/rbf/HIPR2/dilate.htm\n",
        "* https://homepages.inf.ed.ac.uk/rbf/HIPR2/erode.htm\n",
        "* Sobel operator?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvlxkQv883iU",
        "colab_type": "text"
      },
      "source": [
        "Last revised 6/25/2019."
      ]
    }
  ]
}