{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Notes",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Benned-H/Reading_List/blob/master/Notes/NLP_Notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUt7AcGCqhWL",
        "colab_type": "text"
      },
      "source": [
        "# General Notes from Sources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUhJOwGgqsO4",
        "colab_type": "text"
      },
      "source": [
        "## 1. NLP Evaluation\n",
        "Slides by Brendan O'Connor [Link](https://people.cs.umass.edu/~brenocon/inlp2015/15-eval.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBRqpqU7qxmA",
        "colab_type": "text"
      },
      "source": [
        "How can we evaluate NLP systems over the numerous possible tasks, such as classification, translation, or summarization? We should consider metrics, the validity/reliability of human judgment, and also the texts we're testing with.\n",
        "\n",
        "* Extrinsic - Incorporate the system into a downstream task.\n",
        "* Intrinsic - Automatic evaluation based on pre-judged examples or human judgment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5pBVKcEr7qQ",
        "colab_type": "text"
      },
      "source": [
        "**Diagram**: *Contingency Table for Binary Classification*\n",
        "\n",
        "- | Actual + | Actual -\n",
        "--- | --- | ---\n",
        "Pred. + | True Pos. | False Pos.\n",
        "Pred. - | False Neg. | True Neg.\n",
        "\n",
        "$\\text{Precision}=\\frac{\\text{TP}}{\\text{TP}+\\text{FP}}$, or $P(\\text{correct}|\\text{predicted positive})$.\n",
        "\n",
        "$\\text{Recall}=\\frac{\\text{TP}}{\\text{TP}+\\text{FN}}$, or $P(\\text{correct}|\\text{actual positive})$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwXv-AFU16u0",
        "colab_type": "text"
      },
      "source": [
        "**Machine Translation Evaluation** - Additional Info used from [here](https://www.aclweb.org/anthology/W03-2804)\n",
        "\n",
        "The best metric is manual review, but this does require subjective criteria. **Subjective Sentence Error Rate (SSER)** scores sentences on a scale 0 to 10 by quality of translation. A few issues with this metric include its variability (results may vary) and that it weights all sentences, regardless of length, as equally important."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHHhhOR63IEs",
        "colab_type": "text"
      },
      "source": [
        "A more automatic method is **Word Error Rate (WER)**, which is the percentage of words needed to be inserted/deleted/replaced to reach the reference sentence. This forms an 'editing distance' between the sentences. A main drawback is dependence on reference sentences, and that this method only considers *one* valid translation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WqwWbLc31et",
        "colab_type": "text"
      },
      "source": [
        "The **BLEU** Metric calculates its score using N-gram precision by sequences of *n* words. The percentage of n-grams found from the reference is calculated by considering each sequence in the result translation only once per n-gram level. It also considers the length of the output translation (as to avoid a cheating output, eg. \"the\" with precision 1.0)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgQJGAUJ-Rwq",
        "colab_type": "text"
      },
      "source": [
        "So the formula for BLEU is a weighted geometric mean with a brevity penalty. Thus the BLEU4 formula (count n-grams up to length 4):\n",
        "\n",
        "$\\exp(\\sum_{n=1}^42^{1-n}\\log(p_n)-\\max(\\frac{\\text{words in reference}}{\\text{words in machine}} - 1,0))$,   \n",
        "where $p_n$ is the $n$-gram precision. Overall, this metric somewhat predicts human judgements well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUyuK3ZUCezp",
        "colab_type": "text"
      },
      "source": [
        "**Representativeness Considerations**\n",
        "\n",
        "Is the text we have from the right distribution, domain, or genre we care about? Are there enough examples that we trust it? The first of these questions is a judgement call, and the second a statistical question. This is where the slides end, reached on 6/13/2019."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ij5tFHpqqqB",
        "colab_type": "text"
      },
      "source": [
        "# Definitions"
      ]
    }
  ]
}